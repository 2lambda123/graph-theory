# Text Mining

> Is machine learning the best solution to text mining?<br>What if graph theory beats it in both time and space complexity?

The answer is obvious, absolutely not. The graph theory is definitely an underestimated data structure, compared to all the hype of machine learning. First, let's talk about what this project is and later you would realize why sometimes graph structure works better. In this case, graph theory turns out to beat supervised learning in both time and space complexity.

This project is designed to be integrated into my scraping script called <a href=https://github.com/je-suis-tm/web-scraping/blob/master/MENA%20Newsletter.py>MENA Newsletter</a>. Initially, the script would scrape news titles from different mainstream websites (so-called fake news by Mr. Trump :joy:) including BBC, Reuters, Al Jazeera, etc. All these scraped news titles, links and preview images would be concatenated into one HTML email which is automatically sent to my inbox every morning. After a couple of days of experiments, I realized that many websites were actually reporting the same story but in different titles and preview images. It was totally a waste of energy and time to read these similar contents over and over again. And not every piece of information was worth my time to read. Some stories such as 'British Iranian woman got put in jail' was not exactly business related (I'm sorry, it is a great story though). Couldn't I find a way to create a filter to extract the key information?

![alt text](https://github.com/je-suis-tm/graph-theory/blob/master/Text%20Mining%20project/preview/email.PNG)

The first thing came to my mind, of course to anyone, was machine learning, given the halo around it. We could build up a classifier to determine which one is key information. A simple Naive Bayes Multivariate Event Model would do the trick. The methodology is basically based on the assumption that when certain key words such as 'crude', 'south pars' and 'LNG' appear together in a title, the title is more likely to be petroleum business related. It turned out that Bernoulli Naive Bayes Classifier worked pretty well for this sort of key information screening. The accuracy was always capped at 70 to 80 percent. It could be improved by building up a better vocabulary matrix. The downside of this method is the time complexity with metadata. It is always the issue of supervised learning especially if it is generative learning. The default `sklearn` package needs to calculate the conditional probability of each unique word in the vocabulary matrix repeatedly when making a forecast. In terms of the execution speed, the whole process could be boosted with a little computer science technique called memoization. I did write a <a href=https://github.com/je-suis-tm/machine-learning/blob/master/optimized%20naive%20bayes%20with%20memoization.py>self-implementation of Naive Bayes</a> that its time complexity was greatly optimized with big data. Still, that didn't solve the problem of similar contents with different titles. You may argue that I wasn't working hard enough. We could build up a two-dimensional dataframe with the length of n*(n-1)/2 assuming n is how many titles we have scraped. The purpose is to compare all the titles mano-a-mano to determine if they refer to the same event. Yes, we could, and we convert the dataframe into a multi-dimensional vocabulary matrix. <a href=https://github.com/je-suis-tm/machine-learning/blob/master/binary%20support%20vector%20machine.ipynb>Support Vector Machine</a> or Random Forrest or any other classifiers could help us to remove the similar contents and extract the key information. Well, the issue of time and space complexity still exists. We still have to manually classify everything and save a training dataset on a local drive.

Is that the only trick up my sleeves? Never, I am David Copperfield, I walk through the Great Wall and make Statue of Liberty vanish into thin air. Here comes a great one, how about graph theory? We could connect news titles from different sources together on the criteria of how many words they have in common. As usual for natural language processing, we always need to keep a stopword list to exclude some stop words. Stopword refers to words like 'we are' which is common in sentences but does not have an impact on the outcome of NLP. In our case, some country or city names such as 'lebanon', 'iran', 'tehran' or 'beirut' should be included in the stopword list. Unfortunately, the node names of a graph structure cannot be the exact news titles. The package `networkx` does not allow a node name made up of more than fifteen words. In this case, the index of the news title in a concatenated dataframe would be presented as the node name. The edge between two nodes would be established if and only if two news titles share some words in common. The number of common words would be denoted as the weight of the edge. 

But there is another problem, how can we connect word 'walking' with word 'walked'? Well, in that sense, we need stemming and lemmatization. Stemming is to revert a word back to its root. Currently `NLTK` is the most popular NLP package. Even though the famous Porter stemmer is not very effective, as it's a rule-based stemmer. English, unlike its neighbor Fran√ßais, has messy vocabulary rules as many phrases are borrowed from other languages. Keeping a dictionary-based stemmer would require a lot of space. Hence, we would have to cope with the imperfect Porter stemmer, which basically means chopping off the end of a word. Lemmatization is to normalize a word back to its base form. It removes all past simple and past participle form. Wordnet lemmatizer is a relatively better tool than Porter stemmer, though they both have their shortcomings. In general, I prefer to apply lemmatization before stemming to get a clean vocabulary matrix.

Enough chitchat on the theories, let's visualize the graph network and see what is really happening. In the following figure, the color of the edge reflects the weight of the edge. The more words two news titles share in common, the warmer the edge color is. Some titles may not have any word in common with the rest. These lonewolves do not appear in this graph structure. Later on, we could add these rebellious titles back to our output. For the moment, we are only concerned with the nodes in the graph ADT.

![alt text](https://github.com/je-suis-tm/graph-theory/blob/master/Text%20Mining%20project/preview/original.png)

The assumption of our graph theory approach is vital. Assuming a piece of story is a breaking news, it is so important that every mainstream media would cover it. Maybe different media websites cover the story from different perspectives in relatively different titles. The script called MENA newsletter scrapes a lot of major websites. There must be some similar contents with different titles from different sources. These titles should be connected to each other in a graph structure since the similar contents should have at least one key word in common. The common key word could be location, event or even name. For example, we have the following titles, 'Airstrike on Yemen school bus is apparent war crime' from CNN, 'UN accuses Saudi coalition of war crimes in Yemen' from Financial Times and 'Mistakes admitted in Yemen bus attack' from BBC. These titles are connected by the common words 'school bus' and 'war crime' ('yemen' is a stop word so excluded). The content 'yemen bus attack is a war crime' exists in every media website under a different title. We only want to see this story once in our newsletter instead of three times from different sources. The title of the content with the most common words with others is denoted as our target, in this one, 'Airstrike on Yemen school bus is apparent war crime'.

![alt text](https://github.com/je-suis-tm/graph-theory/blob/master/Text%20Mining%20project/preview/edge.png)

In a graph structure, our targets could be presented as key nodes of strongly connected components. In the following visualization, black squares highlight strongly connected components and red circles highlight key nodes which either have the most edges in a given strongly connected component or connect to other strongly connected components.

![alt text](https://github.com/je-suis-tm/graph-theory/blob/master/Text%20Mining%20project/preview/initial%20target.png)

Is there any known traversal algorithm that can return a list of our target nodes? Not to my knowledge (If you do, please feel free to comment). Nevertheless, our selection criteria is not complex and we could always implement our own version of traversal algorithm. The first thing that came to my mind was <a href=https://github.com/je-suis-tm/graph-theory/blob/master/BFS%20DFS%20on%20DCG.ipynb>Breadth First Search</a>. Think of target nodes as parent nodes, all we need to examine is that parent nodes are the nodes with the most edges in any given strong connected component. This BFS, I call it Alternative BFS (`alter_bfs` in the following context), would start at each node in the graph structure. Each starting node is defined as a parent node. It would go one layer deeper to the child nodes. Each traversal from the parent node to all child nodes returns a tree structure. The tree structure would be denoted as a strongly connected component. The `alter_bfs` is designed to select a node with most edges in a given strongly connected component and append the node to an output list. When two nodes have the same number of edges, the algorithm would look at the total sum of weights of each node's edges. The node with highest sum of weights would go to the output list. If the sum of weights cannot select a winner, it is then on a first come first served basis. Whichever node the algorithm travels first would be selected. 

In the following graph, red nodes are the nodes selected by `alter_bfs`. Unfortunately, the visualization layout of `networkx` is random unless we specify the fixed position for each node (which implies a lot of work). I tried my very best to keep all nodes in consistent positions throughout these figures.

![alt text](https://github.com/je-suis-tm/graph-theory/blob/master/Text%20Mining%20project/preview/bfs.png)

If we remove the unselected nodes, we end up with our temporary selection by alternative BFS.

![alt text](https://github.com/je-suis-tm/graph-theory/blob/master/Text%20Mining%20project/preview/bfs%20result.png)

Despite the fact that the temporary selection looks much more elegant than the original structure, we could tell that some nodes are still redundant. Even if we use `set()` function to remove duplicates from our output list, we could still encounter a few strongly connected components. The ideal output should be a few nodes that are completely independent of each other. 
How do we get these false positive? For instance, we have node alpha, beta and gamma selected from the first round of alternative BFS. Node alpha connects to node beta and other unselected nodes. Node alpha has 3 edges and node beta has 4 edges. Node beta connects to node alpha and node gamma. And node gamma has 5 edges and one of them is connected to node beta. When Alternative BFS runs on node alpha, we only keep node beta in check. When `alter_bfs` runs on node beta and gamma, as node gamma has more edges than node beta, we would also append node gamma in the output list. But node beta and node gamma are connected and apparently node gamma is the target node we defined. Thus, we need another round of iteration to remove the redundant.

![alt text](https://github.com/je-suis-tm/graph-theory/blob/master/Text%20Mining%20project/preview/bfs%20demo.png)

How do we get rid of these false positive? The methodology is pretty much the same as Alternative BFS. First, we check if two nodes are connected. If so, we compare the number of edges for each node and remove those which have smaller number of edges. If both nodes have the same number of edges, the total sum of weights of each node would be the selection criteria. Again, if the total sum of weights cannot tell a winner, it is always on a first come first served basis. This process, we call it `remove_child`. As inhuman as it sounds, all it does is merely to extract the key node with most edges from a given strongly connected component. Here comes the humanitarian part. In `remove_child`, there is a single child adoption policy. For each node we would love to kick out, if the ditched node has a child node with only one edge which connected back to the ditched node itself (call it single child node even it has siblings), we would include this single child node as a compensation for dismissing its parent node. The reason behind that is to make sure the single child is being taken care of, which learns from the separation of illegal immigrants and their children at the south border. Okay, I am making bad jokes again :unamused:. As a matter of fact, it ensures the presence of exclusive or niche news content from anonymous tips (minority matters!). When a ditched node has more than one leaf node as single child node, we apply last come first serve to multiple single child case. What we get after first round of `remove_child` is the ultra-high-importance news (we will call it VIP nodes in the following context). In the following figure, VIP nodes are blue crossed, removed nodes are green crossed and single child nodes have no markings.

![alt text](https://github.com/je-suis-tm/graph-theory/blob/master/Text%20Mining%20project/preview/small%20target.png)

Inspite of the fact that this is an elegant traversal algorithm, we could tell that some nodes are still redundant. Even if we use `set()` function to remove duplicates from our output list, we could still end up with a lot of false positive. For instance, we have node alpha, beta and gamma. Node alpha connects to node beta and others. Node alpha has 4 edges and node beta only has 2 edges. Node beta has two edges, which connects to node alpha and node gamma. And node gamma only has one edge connected to node beta. So when Alter BFS runs on node alpha and node beta, we only keep node alpha in check. When Alter BFS runs on node gamma, as node gamma is not connected to node alpha and node beta has more edges than node gamma, we would also append node beta in the output list. But node beta and node alpha are connected and apparently node alpha is the target node we defined. Thus, we need another round of iteration to remove one node from any two connected nodes in the output list. 

The next step is to compare VIP nodes with our temporary selection (the results after the initial Alternative BFS). We need to exclude any nodes that connect to VIP nodes and exist in the temporary selection (allow me to nickname them agent nodes). What is left is VIP nodes and some nodes that do not connect to VIP nodes (they are lucky nodes). Why are lucky nodes eliminated from the initial `remove_child`? Simply because lucky nodes are connected to agent nodes but they have fewer edges than agent nodes. And agent nodes are connected to VIP nodes but they have fewer edges than VIP nodes. Think of the previous sample with alpha, beta and gamma, gamma is VIP node, beta is agent node and alpha is lucky node.

The reason of being called lucky node is they get a second chance. These lucky nodes will need another round of `remove_child` process to make it to the final output. Even though lucky nodes do not connect to VIP nodes, there is a possibility that lucky nodes are connected to each other. The second stage of `remove_child` is to screen out the lucky node with most edges in a given strongly connect component. The ideal output for us is VIP nodes with independent lucky nodes and adopted single child nodes.

![alt text](https://github.com/je-suis-tm/graph-theory/blob/master/Text%20Mining%20project/preview/temp%20result.png)
![alt text](https://raw.githubusercontent.com/je-suis-tm/graph-theory/master/Text%20Mining%20project/preview/remove%20children%20demo.png)

The methodology is pretty much the same as Alter BFS. First, we check if two nodes are connected. If so, we compare the number of edges for each node and remove those which have smaller number of edges. If both nodes have the same number of edges, the total sum of weights of each node would be the selection criteria. Again, if the total sum of weights cannot tell a winner, it is always on a first come first served basis.

Voila! This is the result of our graph-theory-based text mining! Unlike supervised learning, the algorithm doesn't require an extremely large memory for training dataset. It beats machine learning in both time and space complexity.

![alt text](https://github.com/je-suis-tm/graph-theory/blob/master/Text%20Mining%20project/preview/final%20result.png)

Even though this graph traversal algorithm has delivered the key information throughout iterations, there is still work to do. For those nodes not included in our graph ADT, they may be some niche information which is exclusive to one particular source. It is best to append them to the output list. In terms of key information extraction, I personally tune more false positive than false negative for the fear of missing out.

Finally, is this approach flawless? Nope, no algorithm is perfect. As much as I love the algorithm I developed, Alternative BFS could still miss some information from the graph structure. Consider we have two nodes, 'Nazanin Zaghari-Ratcliffe back in Tehran prison' and 'Some 400 prisoners escape prison in Tripoli chaos'. They are connected by the common word 'prison' but they appear to be completely different contents. We may exclude a false negative by accident. I suppose there is always a tradeoff among different algorithms. Additionally, no evidence suggests that machine learning outperforms graph theory in accuracy. Is there any room for improvement? Yes and always. Currently we assume what we scrape is the latest news from different sources. Any involvement of historical archives requires an introduction of a new dimension, time, into the graph structure. Our edge connection based upon common words may sabotage the news timeline without precaution.

In terms of time, space and accuracy, apparently graph structure traversal is a better approach for this scenario rather than machine learning. It answers the question, machine learning is not the silver bullet. Graph theory has the potential to solve sophisticated text mining problems as well.

